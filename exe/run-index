#!/usr/bin/env ruby
# encoding: utf-8

# Index all markdown files in a directory
#
# Usage: run-index config.json
#
# Requires OpenAI API Key stored in DOT_OPENAI_KEY

require "json"
require "digest"
require "stringio"
require "thread"
require "set"

require_relative "../llm/llm"
require_relative "../llm/embedding"
require_relative "../readers/reader"
require_relative "../lib/config_loader"
require_relative "../storage/sqlite_index"

if ARGV.length != 1
    STDOUT << "Invalid arguments received, need a config file\n"
    exit 1
end

begin
    CONFIG = ConfigLoader.load_config(ARGV[0])
rescue => e
    STDOUT << "Config error: #{e.message}\n"
    exit 1
end

OPENAI_KEY = ENV["DOT_OPENAI_KEY"] || ""
GEMINI_KEY = ENV["DOT_GEMINI_KEY"] || ""
OPENROUTER_KEY = ENV["DOT_OPENROUTER_KEY"] || ""

chat_provider = CONFIG.dig(:chat, :provider) || CONFIG.dig("chat", "provider") || 'openai'
embedding_provider = CONFIG.dig(:embedding, :provider) || CONFIG.dig("embedding", "provider") || 'openai'

if (chat_provider.downcase == 'openai' || embedding_provider.downcase == 'openai') && OPENAI_KEY.empty?
    STDOUT << "Remember to set env DOT_OPENAI_KEY\n"
    exit 9
end

if (chat_provider.downcase == 'gemini' || embedding_provider.downcase == 'gemini') && GEMINI_KEY.empty?
    STDOUT << "Remember to set env DOT_GEMINI_KEY\n"
    exit 9
end

if (chat_provider.downcase == 'openrouter' || embedding_provider.downcase == 'openrouter') && OPENROUTER_KEY.empty?
    STDOUT << "Remember to set env DOT_OPENROUTER_KEY\n"
    exit 9
end

def scan_files(path, out, status_cb)
    status_cb.call("Scanning dir")
    name_match = path.nameMatch || "*.{md,markdown}"
    dir_blob = File.join(File.expand_path(path.dir), "**", name_match)
    files = Dir[dir_blob]
    out << "Scan dir: #{dir_blob}, Found: #{files.length}\n"
    files
end

def reader_class_or_exit(path, out)
    reader_class = get_reader(path.reader)
    if reader_class.nil?
        out << "Reader undefinied: #{path.reader}\n"
        exit 9
    end
    reader_class
end

def index_path_jsonl(path, out, status_cb, log_cb)
    out << "Read path name: #{path.name}, reader: #{path.reader}\n"
    status_cb.call("Reading index")

    # Read existing index
    out << "Read existing index: #{path.out}, time: @#{Time.now}\n"
    index_db = {}
    index_file = File.expand_path(path.out)

    File.foreach(index_file) do |line|
        item = JSON.parse(line)
        index_db[item["hash"]] = item
    end if File.exist?(index_file)
    out << "Found index: #{index_db.length}\n"
    files = scan_files(path, out, status_cb)

    # Get reader class
    reader_class = reader_class_or_exit(path, out)

    # Build index
    out << "Building index @#{Time.now}\n["
    status_cb.call("Indexing 0/#{files.length} files, created=0, skipped=0")
    skipped = 0
    created = 0
    File.open(index_file, "w") do |index_newdb|
        files.each_with_index do |file, file_idx|
            status_cb.call("Indexing #{file_idx}/#{files.length} files, created=#{created}, skipped=#{skipped}")
            begin
                chunks = reader_class.new(file).load.chunks
            rescue => e
                out << "\nError reading #{file}: #{e.class}: #{e.message}\n"
                out << "Backtrace: #{e.backtrace&.first}\n"
                log_cb.call("[#{path.name}] Error reading #{file}: #{e.class}: #{e.message}")
                next
            end

            chunks.each_with_index do |chunk, chunk_idx|
                begin
                    hash = Digest::SHA256.hexdigest(chunk)

                    if index_db[hash] # found in old DB
                        index_newdb.puts(index_db[hash].to_json)

                        skipped += 1
                        next
                    end

                    created += 1
                    embedding = embedding(chunk)

                    line = { path: file, hash: hash, chunk: chunk_idx, embedding: embedding }
                    index_newdb.puts(line.to_json)
                rescue => e
                    out << "\nError indexing file=#{file}, chunk=#{chunk_idx}: #{e.class}: #{e.message}\n"
                    out << "Hash: #{hash}\n" if defined?(hash)
                    out << "Backtrace: #{e.backtrace&.first}\n"
                    log_cb.call("[#{path.name}] Error indexing #{file} chunk #{chunk_idx}: #{e.class}: #{e.message}")
                    next
                end
            end

            if file_idx % 50 == 0 # flush the file writes
                index_newdb.flush
                out << file_idx
            else
                out << "."
            end
        end
    end

    out << "]\nDone @#{Time.now}, Created: #{created}, Skipped: #{skipped}\n"
    status_cb.call("Done. created=#{created}, skipped=#{skipped}")
end

def index_path_sqlite(path, out, status_cb, log_cb)
    out << "Read path name: #{path.name}, reader: #{path.reader}\n"
    status_cb.call("Reading index")

    out << "Read existing sqlite index: #{path.db_file}@#{path.db_table}, time: @#{Time.now}\n"
    store = SqliteIndex.new(path.db_file, path.db_table)
    index_db = store.hash_lookup
    out << "Found index: #{index_db.length}\n"

    files = scan_files(path, out, status_cb)
    reader_class = reader_class_or_exit(path, out)

    out << "Building index @#{Time.now}\n["
    status_cb.call("Indexing 0/#{files.length} files, created=0, skipped=0")
    skipped = 0
    created = 0
    scanned_files = Set.new

    begin
        store.transaction do
            files.each_with_index do |file, file_idx|
                status_cb.call("Indexing #{file_idx}/#{files.length} files, created=#{created}, skipped=#{skipped}")
                begin
                    chunks = reader_class.new(file).load.chunks
                rescue => e
                    out << "\nError reading #{file}: #{e.class}: #{e.message}\n"
                    out << "Backtrace: #{e.backtrace&.first}\n"
                    log_cb.call("[#{path.name}] Error reading #{file}: #{e.class}: #{e.message}")
                    next
                end

                valid_chunks = []
                chunks.each_with_index do |chunk, chunk_idx|
                    begin
                        hash = Digest::SHA256.hexdigest(chunk)
                        old = index_db[hash]

                        if old
                            emb = old["embedding"]
                            skipped += 1
                        else
                            emb = embedding(chunk)
                            created += 1
                        end

                        normalized = normalize_embedding(emb)
                        bucket = bucket_key(normalized)

                        store.upsert_chunk(
                            path: file,
                            chunk: chunk_idx,
                            hash: hash,
                            embedding: emb,
                            bucket: bucket,
                            text: chunk
                        )
                        valid_chunks << chunk_idx
                    rescue => e
                        out << "\nError indexing file=#{file}, chunk=#{chunk_idx}: #{e.class}: #{e.message}\n"
                        out << "Hash: #{hash}\n" if defined?(hash)
                        out << "Backtrace: #{e.backtrace&.first}\n"
                        log_cb.call("[#{path.name}] Error indexing #{file} chunk #{chunk_idx}: #{e.class}: #{e.message}")
                        next
                    end
                end

                store.delete_stale_chunks(file, valid_chunks)
                scanned_files.add(file)

                if file_idx % 50 == 0
                    out << file_idx
                else
                    out << "."
                end
            end

            store.delete_stale_paths(scanned_files.to_a)
        end
    ensure
        store.close
    end

    out << "]\nDone @#{Time.now}, Created: #{created}, Skipped: #{skipped}\n"
    status_cb.call("Done. created=#{created}, skipped=#{skipped}")
end

def index_path(path, out, status_cb, log_cb)
    if path.db_file && path.db_table
        index_path_sqlite(path, out, status_cb, log_cb)
    else
        index_path_jsonl(path, out, status_cb, log_cb)
    end
end

status_mutex = Mutex.new
status_map = {}
log_lines = []

status_cb = lambda do |name, msg|
    status_mutex.synchronize { status_map[name] = msg }
end

log_cb = lambda do |line|
    status_mutex.synchronize do
        log_lines << line
        log_lines.shift while log_lines.length > 20
    end
end

printer = Thread.new do
    next unless STDOUT.tty?
    loop do
        lines = nil
        logs = nil
        status_mutex.synchronize do
            lines = status_map.map { |k, v| "#{k}: #{v}" }
            logs = log_lines.dup
        end
        STDOUT << "\e[2J\e[H"
        STDOUT << "Index status (live)\n"
        lines.each { |l| STDOUT << l << "\n" }
        STDOUT << "\nRecent errors\n"
        if logs.empty?
            STDOUT << "(none)\n"
        else
            logs.each { |l| STDOUT << l << "\n" }
        end
        STDOUT.flush
        sleep 0.2
    end
end

threads = CONFIG.paths.map do |path|
    Thread.new do
        buffer = StringIO.new
        begin
            status_cb.call(path.name, "Starting")
            index_path(path, buffer, ->(msg) { status_cb.call(path.name, msg) }, log_cb)
        rescue => e
            buffer << "\nThread error: #{e.class}: #{e.message}\n"
            buffer << "Backtrace: #{e.backtrace&.first}\n"
        ensure
            Thread.current[:output] = buffer.string
            Thread.current[:path_name] = path.name
            status_cb.call(path.name, "Finished")
        end
    end
end

threads.each do |t|
    t.join
end

printer.kill if printer&.alive?

threads.each do |t|
    name = t[:path_name] || "path"
    STDOUT << "\n=== Index output: #{name} ===\n"
    STDOUT << (t[:output] || "")
    STDOUT << "\n=== End: #{name} ===\n"
end
