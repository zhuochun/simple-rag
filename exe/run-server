#!/usr/bin/env ruby
# encoding: utf-8

# Query and answer questions based on an index file
#
# Usage: run-server config.json
#
# Requires OpenAI API Key stored in DOT_OPENAI_KEY

require "json"
require "set"
require "sinatra/base"

require_relative "../server/retriever"
require_relative "../server/synthesizer"
require_relative "../server/discuss"
require_relative "../server/duplicate"
require_relative "../server/article"
require_relative "../lib/config_loader"
require_relative "../lib/provider_env_validator"

if ARGV.length != 1
    STDOUT << "Invalid arguments received, need a config file\n"
    exit 1
end

begin
    CONFIG = ConfigLoader.load_config(ARGV[0], with_path_map: true)
rescue => e
    STDOUT << "Config error: #{e.message}\n"
    exit 1
end

DISMISSED_FILE = CONFIG.dismissedDuplicatesFile
DISMISSED_CLUSTERS = if DISMISSED_FILE && File.exist?(DISMISSED_FILE)
    Set.new(File.read(DISMISSED_FILE).split(/\n+/))
else
    Set.new
end

def save_dismissed
    return unless DISMISSED_FILE
    File.open(DISMISSED_FILE, "w") do |f|
        DISMISSED_CLUSTERS.each { |id| f.puts(id) }
    end
end

def resolve_lookup_paths(selected, default_to_search_default: false)
    names = Array(selected).compact

    if names.empty?
        if default_to_search_default
            names = CONFIG.paths.select { |p| p.searchDefault }.map(&:name)
            names = CONFIG.path_map.keys if names.empty?
        else
            names = CONFIG.paths.map(&:name)
        end
    end

    names.map { |name| CONFIG.path_map[name] }.compact
end

def merge_entries_by_chunk(entries)
    unique = {}

    entries.each do |entry|
        key = [entry["path"], entry["chunk"]]
        if unique[key]
            unique[key]["score"] = (unique[key]["score"] || 0.0) + (entry["score"] || 0.0)
        else
            unique[key] = entry
        end
    end

    unique.values
end

def min_max_normalize_score(score, min_score, max_score)
    value = score.to_f
    if max_score > min_score
        (value - min_score) / (max_score - min_score)
    elsif value > 0
        1.0
    else
        0.0
    end
end

def fuse_entries_with_weighted_rrf(lists, component_weights: { weighted: 0.7, rrf: 0.3 }, rrf_k: 60)
    merged = {}

    lists.each do |list|
        entries = Array(list[:entries])
        next if entries.empty?

        list_weight = list[:weight].to_f
        ranked = entries.sort_by { |item| -(item["score"] || 0.0) }
        scores = ranked.map { |item| item["score"].to_f }
        min_score = scores.min || 0.0
        max_score = scores.max || 0.0

        ranked.each_with_index do |item, idx|
            key = [item["path"], item["chunk"]]
            row = merged[key]
            unless row
                row = item.dup
                row["_weighted_score"] = 0.0
                row["_rrf_score"] = 0.0
                merged[key] = row
            end

            normalized = min_max_normalize_score(item["score"], min_score, max_score)
            rank = idx + 1
            row["_weighted_score"] += list_weight * normalized
            row["_rrf_score"] += list_weight * (1.0 / (rrf_k + rank))
        end
    end

    rows = merged.values
    return rows if rows.empty?

    max_weighted = rows.max_by { |r| r["_weighted_score"] }["_weighted_score"].to_f
    max_rrf = rows.max_by { |r| r["_rrf_score"] }["_rrf_score"].to_f

    weighted_ratio = component_weights[:weighted].to_f
    rrf_ratio = component_weights[:rrf].to_f

    rows.each do |row|
        weighted_norm = max_weighted > 0 ? row["_weighted_score"].to_f / max_weighted : 0.0
        rrf_norm = max_rrf > 0 ? row["_rrf_score"].to_f / max_rrf : 0.0
        row["score"] = (weighted_ratio * weighted_norm) + (rrf_ratio * rrf_norm)
        row.delete("_weighted_score")
        row.delete("_rrf_score")
    end

    rows.sort_by { |item| -(item["score"] || 0.0) }
end

def top_n_by_score(entries, top_n)
    n = top_n.to_i
    return [] if n <= 0 || entries.empty?

    if entries.length <= n
        return entries.sort_by { |item| -(item["score"] || 0.0) }
    end

    entries.max_by(n) { |item| item["score"] || 0.0 }
           .sort_by { |item| -(item["score"] || 0.0) }
end

def serialize_entries(entries)
    reader_cache = {}

    entries.map do |item|
        reader = reader_cache[item["path"]] ||= item["reader"].load
        {
            path: item["path"],
            lookup: item["lookup"],
            id: item["id"],
            url: item["url"],
            text: reader.get_chunk(item["chunk"]),
            score: item["score"],
        }
    end
end

OPENAI_KEY = ENV["DOT_OPENAI_KEY"] || ""
GEMINI_KEY = ENV["DOT_GEMINI_KEY"] || ""
OPENROUTER_KEY = ENV["DOT_OPENROUTER_KEY"] || ""

missing_key_msg = ProviderEnvValidator.missing_key_message(CONFIG)
if missing_key_msg
    STDOUT << "#{missing_key_msg}\n"
    exit 9
end

class SimpleRagServer < Sinatra::Application
    # list all the paths that can be searched
    get "/paths" do
        content_type :json

        CONFIG.paths.map { |p| { name: p.name, searchDefault: p.searchDefault } }.to_json
    end

    # query within the paths
    post "/q" do
        content_type :json

        data = JSON.parse(request.body.read)
        lookup_paths = resolve_lookup_paths(data["paths"], default_to_search_default: true)

        top_n = (data["topN"] || 20).to_i
        q = data["q"]

        entries = retrieve_by_embedding(lookup_paths, q)
        if q.to_s.strip.length < 5 && q.to_s.split(/\s+/).length < 5
            entries.concat(retrieve_by_text(lookup_paths, q))
            entries = merge_entries_by_chunk(entries)
        end

        ordered = top_n_by_score(entries, top_n)
        { data: serialize_entries(ordered) }.to_json
    end

    # agentic query - expand the query using LLM before searching
    post "/q_plus" do
        content_type :json

        data = JSON.parse(request.body.read)
        lookup_paths = resolve_lookup_paths(data["paths"], default_to_search_default: true)
        top_n = (data["topN"] || 20).to_i

        expanded_q = expand_query(data["q"])
        variants = expand_variants(data["q"])

        store_cache = {}
        base_entries = []
        expanded_entries = []
        variant_entries = []
        begin
            base_entries = retrieve_by_embedding(lookup_paths, data["q"], store_cache: store_cache)
            expanded_entries = retrieve_by_embedding(lookup_paths, expanded_q, store_cache: store_cache)
            variant_entries = retrieve_by_text(lookup_paths, variants, store_cache: store_cache) unless variants.empty?
        ensure
            close_store_cache(store_cache)
        end

        fused = fuse_entries_with_weighted_rrf(
            [
                { name: "base", entries: base_entries, weight: 1.0 },
                { name: "expanded", entries: expanded_entries, weight: 0.85 },
                { name: "variants", entries: variant_entries, weight: 0.75 },
            ],
            component_weights: { weighted: 0.7, rrf: 0.3 },
            rrf_k: 60
        )
        ordered = top_n_by_score(fused, top_n)

        {
            data: serialize_entries(ordered),
            expanded: expanded_q,
            variants: variants,
        }.to_json
    end

    # synthesize notes into a summary
    post "/synthesize" do
        content_type :json

        data = JSON.parse(request.body.read)
        summary = synthesize_notes(data["notes"])

        { note: summary }.to_json
    end

    # generate discussion for a single note
    post "/discuss" do
        content_type :json

        data = JSON.parse(request.body.read)
        discussion = discuss_note(data["note"], data["query"])

        { discussion: discussion }.to_json
    end

    # find duplicate notes across selected paths
    post "/duplicates" do
        content_type :json

        data = JSON.parse(request.body.read)
        threshold = (data["threshold"] || 0.9).to_f
        lookup_paths = resolve_lookup_paths(data["paths"])

        clusters = find_duplicates(lookup_paths, threshold)
        clusters = clusters.map { |c| { id: cluster_key(c), items: c } }
        clusters.reject! { |c| DISMISSED_CLUSTERS.include?(c[:id]) }

        { clusters: clusters }.to_json
    end

    # dismiss a duplicate cluster
    post "/dismiss" do
        content_type :json

        data = JSON.parse(request.body.read)
        if data["id"]
            DISMISSED_CLUSTERS.add(data["id"])
            save_dismissed
        end

        { ok: true }.to_json
    end

    # return random notes from selected paths
    post "/random" do
        content_type :json

        data = JSON.parse(request.body.read)
        count = (data["count"] || 3).to_i
        lookup_paths = resolve_lookup_paths(data["paths"])

        samples = []
        lookup_paths.each do |p|
            reader_cls = get_reader(p.reader)
            next unless reader_cls

            file_cache = {}
            if p.db_file && p.db_table
                with_sqlite_store(p) do |store|
                    store.random_chunk_refs(count).each do |it|
                        reader = file_cache[it["path"]] ||= reader_cls.new(it["path"]).load
                        samples << {
                            path: p.name,
                            lookup: p.name,
                            id: extract_id(it["path"]),
                            url: extract_url(it["path"], p.url),
                            text: reader.get_chunk(it["chunk"]),
                            score: 0,
                        }
                    end
                end
                next
            end

            index = load_index_cache(p)
            next unless index

            index.items.sample(count).each do |it|
                reader = file_cache[it[:path]] ||= reader_cls.new(it[:path]).load
                samples << {
                    path: p.name,
                    lookup: p.name,
                    id: extract_id(it[:path]),
                    url: extract_url(it[:path], p.url),
                    text: reader.get_chunk(it[:chunk]),
                    score: 0,
                }
            end
        end

        { data: samples.sample(count) }.to_json
    end

    # retrieve notes similar to provided note text
    post "/similar" do
        content_type :json

        data = JSON.parse(request.body.read)
        lookup_paths = resolve_lookup_paths(data["paths"])
        top_n = (data["topN"] || 3).to_i

        entries = retrieve_by_embedding(lookup_paths, data["note"] || "")
        ordered = top_n_by_score(entries, top_n)

        { data: serialize_entries(ordered) }.to_json
    end

    # read a URL, extract concepts and compare with existing notes
    post "/read_url" do
        content_type :json

        data = JSON.parse(request.body.read)
        url = data["url"]
        lookup_paths = resolve_lookup_paths(data["paths"])

        article = fetch_article(url) rescue ""
        extraction = extract_article(article)
        groups = split_extraction_groups(extraction)

        retrievals = []
        notes = []
        notes_seen = Set.new
        reader_cache = {}

        store_cache = {}
        begin
            groups.each do |group|
                entries = retrieve_by_embedding(lookup_paths, group, store_cache: store_cache)
                ordered = top_n_by_score(entries, 5)

                retrievals << ordered.map do |item|
                    reader = reader_cache[item["path"]] ||= item["reader"].load
                    text = reader.get_chunk(item["chunk"])
                    if text && notes_seen.add?(text)
                        notes << text
                    end

                    {
                        path: item["path"],
                        lookup: item["lookup"],
                        id: item["id"],
                        url: item["url"],
                        text: text,
                        score: item["score"],
                    }
                end
            end
        ensure
            close_store_cache(store_cache)
        end

        argument = argue_new_content(notes, extraction)
        save_article_result(url, article, extraction, argument)

        { extraction: extraction, argument: argument, retrievals: retrievals }.to_json
    end
end

SimpleRagServer.run!
