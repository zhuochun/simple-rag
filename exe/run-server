#!/usr/bin/env ruby
# encoding: utf-8

# Query and answer questions based on an index file
#
# Usage: run-server config.json
#
# Requires OpenAI API Key stored in DOT_OPENAI_KEY

require "json"
require "ostruct"
require 'sinatra/base'

require_relative "../server/retriever"
require_relative "../server/synthesizer"
require_relative "../server/discuss"
require_relative "../server/duplicate"
require_relative "../server/article"

if ARGV.length != 1
    STDOUT << "Invalid arguments received, need a config file\n"
    exit 1
end

config = JSON.parse(File.read(ARGV[0]))
CONFIG = OpenStruct.new(config)
CONFIG.paths = CONFIG.paths.map { |p| OpenStruct.new(p) }
CONFIG.paths.each { |p| p.searchDefault = !!p.searchDefault }
CONFIG.path_map = {}
CONFIG.paths.each { |p| CONFIG.path_map[p.name] = p }
require 'set'
DISMISSED_FILE = CONFIG.dismissedDuplicatesFile
DISMISSED_CLUSTERS = if DISMISSED_FILE && File.exist?(DISMISSED_FILE)
    Set.new(File.read(DISMISSED_FILE).split(/\n+/))
else
    Set.new
end

def save_dismissed
    return unless DISMISSED_FILE
    File.open(DISMISSED_FILE, 'w') do |f|
        DISMISSED_CLUSTERS.each { |id| f.puts(id) }
    end
end


OPENAI_KEY = ENV["DOT_OPENAI_KEY"] || ""
if OPENAI_KEY.empty?
    STDOUT << "Remember to set env DOT_OPENAI_KEY\n"
    exit 9
end

class SimpleRagServer < Sinatra::Application
    # list all the paths that can be searched
    get '/paths' do
        content_type :json

        resp = []
        CONFIG.paths.each do |p|
            resp << { name: p.name, searchDefault: p.searchDefault }
        end
        resp.to_json
    end

    # query within the paths
    post '/q' do
        content_type :json

        data = JSON.parse(request.body.read)

        selected = data["paths"]
        if !selected || selected.empty?
            selected = CONFIG.paths.select { |p| p.searchDefault }.map(&:name)
            selected = CONFIG.path_map.keys if selected.empty?
        end
        lookup_paths = selected.map { |name| CONFIG.path_map[name] }

        topN = (data["topN"] || 20).to_i

        q = data["q"]
        entries = retrieve_by_embedding(lookup_paths, q)
        if q.to_s.strip.length < 5 && q.to_s.split(/\s+/).length < 5
            entries.concat(retrieve_by_text(lookup_paths, q))

            unique = {}
            entries.each do |e|
                key = [e["path"], e["chunk"]]
                if unique[key]
                    unique[key]["score"] = (unique[key]["score"] || 0) + (e["score"] || 0)
                else
                    unique[key] = e
                end
            end

            entries = unique.values
        end
        entries = entries.sort_by { |item| -item["score"] }.take(topN)

        resp = {
            data: [],
        }

        entries.each do |item|
            resp[:data] << {
                path: item["path"],
                lookup: item["lookup"],
                id: item["id"],
                url: item["url"],
                text: item["reader"].load.get_chunk(item["chunk"]),
                score: item["score"],
            }
        end

        resp.to_json
    end

    # agentic query - expand the query using LLM before searching
    post '/q_plus' do
        content_type :json

        data = JSON.parse(request.body.read)

        selected = data["paths"]
        if !selected || selected.empty?
            selected = CONFIG.paths.select { |p| p.searchDefault }.map(&:name)
            selected = CONFIG.path_map.keys if selected.empty?
        end
        lookup_paths = selected.map { |name| CONFIG.path_map[name] }

        topN = (data["topN"] || 20).to_i

        expanded_q = expand_query(data["q"])
        variants = expand_variants(data["q"])

        entries = []
        entries.concat(retrieve_by_embedding(lookup_paths, data["q"]))
        entries.concat(retrieve_by_embedding(lookup_paths, expanded_q))
        variants.each { |v| entries.concat(retrieve_by_text(lookup_paths, v)) }

        unique = {}
        entries.each do |e|
            key = [e["path"], e["chunk"]]
            if unique[key]
                unique[key]["score"] = (unique[key]["score"] || 0) + (e["score"] || 0)
            else
                unique[key] = e
            end
        end

        ordered = unique.values.sort_by { |item| -item["score"] }.take(topN)

        resp = {
            data: [],
            expanded: expanded_q,
            variants: variants,
        }

        ordered.each do |item|
            resp[:data] << {
                path: item["path"],
                lookup: item["lookup"],
                id: item["id"],
                url: item["url"],
                text: item["reader"].load.get_chunk(item["chunk"]),
                score: item["score"],
            }
        end

        resp.to_json
    end

    # synthesize notes into a summary
    post '/synthesize' do
        content_type :json

        data = JSON.parse(request.body.read)

        summary = synthesize_notes(data["notes"])

        { note: summary }.to_json
    end

    # generate discussion for a single note
    post '/discuss' do
        content_type :json

        data = JSON.parse(request.body.read)

        discussion = discuss_note(data["note"])

        { discussion: discussion }.to_json
    end

    # find duplicate notes across selected paths
    post '/duplicates' do
        content_type :json

        data = JSON.parse(request.body.read)

        selected = data["paths"]
        threshold = (data["threshold"] || 0.9).to_f
        if !selected || selected.empty?
            selected = CONFIG.paths.map(&:name)
        end
        lookup_paths = selected.map { |name| CONFIG.path_map[name] }

        clusters = find_duplicates(lookup_paths, threshold)

        clusters = clusters.map { |c| { id: cluster_key(c), items: c } }
        clusters.reject! { |c| DISMISSED_CLUSTERS.include?(c[:id]) }

        { clusters: clusters }.to_json
    end

    # dismiss a duplicate cluster
    post '/dismiss' do
        content_type :json

        data = JSON.parse(request.body.read)
        if data['id']
            DISMISSED_CLUSTERS.add(data['id'])
            save_dismissed
        end
        { ok: true }.to_json
    end

    # return random notes from selected paths
    post '/random' do
        content_type :json

        data = JSON.parse(request.body.read)

        selected = data["paths"]
        count = (data["count"] || 3).to_i
        if !selected || selected.empty?
            selected = CONFIG.paths.map(&:name)
        end
        lookup_paths = selected.map { |name| CONFIG.path_map[name] }

        samples = []
        lookup_paths.each do |p|
            index = load_index_cache(p)
            next unless index
            reader_cls = index.reader_cls
            next unless reader_cls

            index.items.sample(count).each do |it|
                samples << {
                    path: p.name,
                    lookup: p.name,
                    id: extract_id(it[:path]),
                    url: extract_url(it[:path], p.url),
                    text: reader_cls.new(it[:path]).load.get_chunk(it[:chunk]),
                    score: 0,
                }
            end
        end

        samples = samples.sample(count)

        { data: samples }.to_json
    end

    # retrieve notes similar to provided note text
    post '/similar' do
        content_type :json

        data = JSON.parse(request.body.read)

        selected = data["paths"]
        topN = (data["topN"] || 3).to_i
        if !selected || selected.empty?
            selected = CONFIG.paths.map(&:name)
        end
        lookup_paths = selected.map { |name| CONFIG.path_map[name] }

        entries = retrieve_by_embedding(lookup_paths, data["note"] || "")
        entries = entries.sort_by { |item| -item["score"] }.take(topN)

        resp = { data: [] }
        entries.each do |item|
            resp[:data] << {
                path: item["path"],
                lookup: item["lookup"],
                id: item["id"],
                url: item["url"],
                text: item["reader"].load.get_chunk(item["chunk"]),
                score: item["score"],
            }
        end

        resp.to_json
    end

    # read a URL, extract concepts and compare with existing notes
    post '/read_url' do
        content_type :json

        data = JSON.parse(request.body.read)
        url = data['url']

        selected = data['paths']
        if !selected || selected.empty?
            selected = CONFIG.paths.map(&:name)
        end
        lookup_paths = selected.map { |name| CONFIG.path_map[name] }

        article = fetch_article(url) rescue ''
        extraction = extract_article(article)
        groups = split_extraction_groups(extraction)

        retrievals = []
        notes = []
        groups.each do |g|
            entries = retrieve_by_embedding(lookup_paths, g)
            entries = entries.sort_by { |item| -item['score'] }.take(5)

            retrievals << entries.map do |item|
                text = item['reader'].load.get_chunk(item['chunk'])
                notes << text
                {
                    path: item['path'],
                    lookup: item['lookup'],
                    id: item['id'],
                    url: item['url'],
                    text: text,
                    score: item['score'],
                }
            end
        end

        argument = argue_new_content(notes, extraction)

        save_article_result(url, article, extraction, argument)

        { extraction: extraction, argument: argument, retrievals: retrievals }.to_json
    end
end

SimpleRagServer.run!
