#!/usr/bin/env ruby
# encoding: utf-8

# Query and answer questions based on an index file
#
# Usage: run-server config.json
#
# Requires OpenAI API Key stored in DOT_OPENAI_KEY

require "json"
require "set"
require "sinatra/base"

require_relative "../server/retriever"
require_relative "../server/synthesizer"
require_relative "../server/discuss"
require_relative "../server/duplicate"
require_relative "../server/article"
require_relative "../lib/config_loader"
require_relative "../lib/provider_env_validator"

if ARGV.length != 1
    STDOUT << "Invalid arguments received, need a config file\n"
    exit 1
end

begin
    CONFIG = ConfigLoader.load_config(ARGV[0], with_path_map: true)
rescue => e
    STDOUT << "Config error: #{e.message}\n"
    exit 1
end

DISMISSED_FILE = CONFIG.dismissedDuplicatesFile
DISMISSED_CLUSTERS = if DISMISSED_FILE && File.exist?(DISMISSED_FILE)
    Set.new(File.read(DISMISSED_FILE).split(/\n+/))
else
    Set.new
end

def save_dismissed
    return unless DISMISSED_FILE
    File.open(DISMISSED_FILE, "w") do |f|
        DISMISSED_CLUSTERS.each { |id| f.puts(id) }
    end
end

def resolve_lookup_paths(selected, default_to_search_default: false)
    names = Array(selected).compact

    if names.empty?
        if default_to_search_default
            names = CONFIG.paths.select { |p| p.searchDefault }.map(&:name)
            names = CONFIG.path_map.keys if names.empty?
        else
            names = CONFIG.paths.map(&:name)
        end
    end

    names.map { |name| CONFIG.path_map[name] }.compact
end

def merge_entries_by_chunk(entries)
    unique = {}

    entries.each do |entry|
        key = [entry["path"], entry["chunk"]]
        if unique[key]
            unique[key]["score"] = (unique[key]["score"] || 0.0) + (entry["score"] || 0.0)
        else
            unique[key] = entry
        end
    end

    unique.values
end

def top_n_by_score(entries, top_n)
    n = top_n.to_i
    return [] if n <= 0 || entries.empty?

    if entries.length <= n
        return entries.sort_by { |item| -(item["score"] || 0.0) }
    end

    entries.max_by(n) { |item| item["score"] || 0.0 }
           .sort_by { |item| -(item["score"] || 0.0) }
end

def serialize_entries(entries)
    reader_cache = {}

    entries.map do |item|
        reader = reader_cache[item["path"]] ||= item["reader"].load
        {
            path: item["path"],
            lookup: item["lookup"],
            id: item["id"],
            url: item["url"],
            text: reader.get_chunk(item["chunk"]),
            score: item["score"],
        }
    end
end

OPENAI_KEY = ENV["DOT_OPENAI_KEY"] || ""
GEMINI_KEY = ENV["DOT_GEMINI_KEY"] || ""
OPENROUTER_KEY = ENV["DOT_OPENROUTER_KEY"] || ""

missing_key_msg = ProviderEnvValidator.missing_key_message(CONFIG)
if missing_key_msg
    STDOUT << "#{missing_key_msg}\n"
    exit 9
end

class SimpleRagServer < Sinatra::Application
    # list all the paths that can be searched
    get "/paths" do
        content_type :json

        CONFIG.paths.map { |p| { name: p.name, searchDefault: p.searchDefault } }.to_json
    end

    # query within the paths
    post "/q" do
        content_type :json

        data = JSON.parse(request.body.read)
        lookup_paths = resolve_lookup_paths(data["paths"], default_to_search_default: true)

        top_n = (data["topN"] || 20).to_i
        q = data["q"]

        entries = retrieve_by_embedding(lookup_paths, q)
        if q.to_s.strip.length < 5 && q.to_s.split(/\s+/).length < 5
            entries.concat(retrieve_by_text(lookup_paths, q))
            entries = merge_entries_by_chunk(entries)
        end

        ordered = top_n_by_score(entries, top_n)
        { data: serialize_entries(ordered) }.to_json
    end

    # agentic query - expand the query using LLM before searching
    post "/q_plus" do
        content_type :json

        data = JSON.parse(request.body.read)
        lookup_paths = resolve_lookup_paths(data["paths"], default_to_search_default: true)
        top_n = (data["topN"] || 20).to_i

        expanded_q = expand_query(data["q"])
        variants = expand_variants(data["q"])

        store_cache = {}
        entries = []
        begin
            entries.concat(retrieve_by_embedding(lookup_paths, data["q"], store_cache: store_cache))
            entries.concat(retrieve_by_embedding(lookup_paths, expanded_q, store_cache: store_cache))
            entries.concat(retrieve_by_text(lookup_paths, variants, store_cache: store_cache)) unless variants.empty?
        ensure
            close_store_cache(store_cache)
        end

        ordered = top_n_by_score(merge_entries_by_chunk(entries), top_n)

        {
            data: serialize_entries(ordered),
            expanded: expanded_q,
            variants: variants,
        }.to_json
    end

    # synthesize notes into a summary
    post "/synthesize" do
        content_type :json

        data = JSON.parse(request.body.read)
        summary = synthesize_notes(data["notes"])

        { note: summary }.to_json
    end

    # generate discussion for a single note
    post "/discuss" do
        content_type :json

        data = JSON.parse(request.body.read)
        discussion = discuss_note(data["note"], data["query"])

        { discussion: discussion }.to_json
    end

    # find duplicate notes across selected paths
    post "/duplicates" do
        content_type :json

        data = JSON.parse(request.body.read)
        threshold = (data["threshold"] || 0.9).to_f
        lookup_paths = resolve_lookup_paths(data["paths"])

        clusters = find_duplicates(lookup_paths, threshold)
        clusters = clusters.map { |c| { id: cluster_key(c), items: c } }
        clusters.reject! { |c| DISMISSED_CLUSTERS.include?(c[:id]) }

        { clusters: clusters }.to_json
    end

    # dismiss a duplicate cluster
    post "/dismiss" do
        content_type :json

        data = JSON.parse(request.body.read)
        if data["id"]
            DISMISSED_CLUSTERS.add(data["id"])
            save_dismissed
        end

        { ok: true }.to_json
    end

    # return random notes from selected paths
    post "/random" do
        content_type :json

        data = JSON.parse(request.body.read)
        count = (data["count"] || 3).to_i
        lookup_paths = resolve_lookup_paths(data["paths"])

        samples = []
        lookup_paths.each do |p|
            reader_cls = get_reader(p.reader)
            next unless reader_cls

            file_cache = {}
            if p.db_file && p.db_table
                with_sqlite_store(p) do |store|
                    store.random_chunk_refs(count).each do |it|
                        reader = file_cache[it["path"]] ||= reader_cls.new(it["path"]).load
                        samples << {
                            path: p.name,
                            lookup: p.name,
                            id: extract_id(it["path"]),
                            url: extract_url(it["path"], p.url),
                            text: reader.get_chunk(it["chunk"]),
                            score: 0,
                        }
                    end
                end
                next
            end

            index = load_index_cache(p)
            next unless index

            index.items.sample(count).each do |it|
                reader = file_cache[it[:path]] ||= reader_cls.new(it[:path]).load
                samples << {
                    path: p.name,
                    lookup: p.name,
                    id: extract_id(it[:path]),
                    url: extract_url(it[:path], p.url),
                    text: reader.get_chunk(it[:chunk]),
                    score: 0,
                }
            end
        end

        { data: samples.sample(count) }.to_json
    end

    # retrieve notes similar to provided note text
    post "/similar" do
        content_type :json

        data = JSON.parse(request.body.read)
        lookup_paths = resolve_lookup_paths(data["paths"])
        top_n = (data["topN"] || 3).to_i

        entries = retrieve_by_embedding(lookup_paths, data["note"] || "")
        ordered = top_n_by_score(entries, top_n)

        { data: serialize_entries(ordered) }.to_json
    end

    # read a URL, extract concepts and compare with existing notes
    post "/read_url" do
        content_type :json

        data = JSON.parse(request.body.read)
        url = data["url"]
        lookup_paths = resolve_lookup_paths(data["paths"])

        article = fetch_article(url) rescue ""
        extraction = extract_article(article)
        groups = split_extraction_groups(extraction)

        retrievals = []
        notes = []
        notes_seen = Set.new
        reader_cache = {}

        store_cache = {}
        begin
            groups.each do |group|
                entries = retrieve_by_embedding(lookup_paths, group, store_cache: store_cache)
                ordered = top_n_by_score(entries, 5)

                retrievals << ordered.map do |item|
                    reader = reader_cache[item["path"]] ||= item["reader"].load
                    text = reader.get_chunk(item["chunk"])
                    if text && notes_seen.add?(text)
                        notes << text
                    end

                    {
                        path: item["path"],
                        lookup: item["lookup"],
                        id: item["id"],
                        url: item["url"],
                        text: text,
                        score: item["score"],
                    }
                end
            end
        ensure
            close_store_cache(store_cache)
        end

        argument = argue_new_content(notes, extraction)
        save_article_result(url, article, extraction, argument)

        { extraction: extraction, argument: argument, retrievals: retrievals }.to_json
    end
end

SimpleRagServer.run!
